import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

x=torch.empty(5,3)
print(x)
x=torch.rand(5,3)
print(x)
x=torch.zeros(5,3,dtype=torch.long)
print(x)
x=torch.tensor([5.5,3])
x=x.new_ones(5,3,dtype=torch.double)
print(x)
x=torch.randn_like(x,dtype=torch.float)
print(x,x.size())
print(x[:,1])
y=torch.rand(5,3)
print(x+y,torch.add(x,y))
x=torch.randn(4,4)
y=x.view(16)
z=x.view(-1,8)
print(x.size(),y.size(),z.size())
x=torch.randn(1)
print(x)
print(x.item())
x=torch.ones(2,2,requires_grad=True)
print(x)
y=x+2
print(y)
print(y.grad_fn)
z=y*y*3
out=z.mean()
print(z,out)
a=torch.randn(2,2)
a=((a*3)/(a-1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b=(a*a).sum()
print(b.grad_fn)
out.backward()
print(x.grad)
a=torch.tensor(2.2)
print(a.shape,a.size(),len(a.shape))
a=torch.tensor([1.1,2.2])
print(a)
a=torch.FloatTensor(1)
print(a)
a=torch.FloatTensor(2)
print(a)
data=np.ones(2)
a=torch.from_numpy(data)
print(a)
a=torch.tensor([2,3])
print(a)
# 定义数据用以下两种方式
a=torch.Tensor([2,3])
print(a)
a=torch.FloatTensor(2,3)
print(a,a.size())

print(torch.randint(1,100,[4,4]))
print(torch.normal(mean=torch.full([10],0),std=torch.arange(1,0,-0.1)))

print(torch.tensor(np.random.randn(2,5)))

print(torch.linspace(0,10,steps=10))
print(torch.linspace(0,10,steps=11))
print(torch.logspace(0,-1,steps=10))
print(torch.eye(3,4))
print(torch.eye(4))
print(torch.randperm(10))
a=torch.randn(4,3,28,28)
print(a.index_select(0,torch.arange(3)).shape)
print(a.index_select(1,torch.arange(3)).shape)
print(torch.arange(10))

x=torch.randn(3,4)
mask=x.ge(0.5)
a=torch.masked_select(x,mask)
print(mask,a)
x=x.view(-1,6)
print(x.size(),x)
x=x.reshape(4,-1)
print(x)
x=torch.randn(3,4)
print(x.unsqueeze(0).shape)
print(x.unsqueeze(-1).shape)
x=torch.randn(1,2,3,4,1)
print(x.squeeze().shape)
a=torch.randn(4,32,14,14)
b=torch.randn(1,32,1,1)
b=b.expand(4,32,14,14)
print(b.shape)
b=torch.randn(1,32,1,1)
# -1表示该维度不进行拓展
b=b.expand(4,32,-1,-1)
print(b.shape)
b=torch.randn(1,32,1,1)
# 会进行内存的复制
print(b.repeat(4,32,1,1).shape)
a=torch.randn(3,4)
print(a.t())
# [b,c,h,w]
a=torch.randn(4,3,28,32)
# [b,c,h,w]=>[b,g,w,c]
b=a.permute(0,2,3,1)
print(b.shape)

print(b.shape)
b=torch.randn(4,4)
print(b)
print(b+torch.randn(1,4))
a1=torch.rand(4,3,32,32)
a2=torch.rand(5,3,32,32)
print(torch.cat([a1,a2],dim=0).shape)
a1=torch.rand(4,3)
a2=torch.rand(4,3)
# 产生新的维度
print(torch.stack([a1,a2]).shape)
a=torch.rand(5,32,32)
c1,c2,c3=a.split([3,1,1],dim=0)
print(c1.size(),c2.size(),c3.size())
c=a.split(1,dim=0)
print(len(c))
c1,c2=a.chunk(2,dim=0)
print(c1.shape,c2.shape)
a=torch.rand(3,4)
b=torch.rand(4)
print(torch.all(torch.eq(a-b,torch.sub(a,b))))
print(torch.all(torch.eq(a*b,torch.mul(a,b))))
print(torch.all(torch.eq(a/b,torch.div(a,b))))
a=torch.full([4,4],2.7183)
print(a.rsqrt())
print(torch.exp(a))
print(torch.log(a))
a=torch.tensor(3.14)
print(a.floor(),a.ceil(),a.trunc(),a.frac())
grad=torch.rand(3,4)*15
print(grad.max(),grad.median())
# (min)
print(grad.clamp(10))
# (min,max)
print(grad.clamp(0,10))
a=torch.full([8],1)
b=a.reshape(2,4)
c=a.reshape(2,2,2)
print(a.norm(1),b.norm(1),c.norm(1))
print(b.norm(2,dim=1))
print(c.norm(2,dim=0))
a=torch.rand(4,10)
print(a.max(dim=1))
print(a.argmax(dim=1))
print(a.max(dim=1,keepdim=True))
print(a.argmax(dim=1,keepdim=True))
print(a.topk(3,dim=1))
print(a.topk(3,dim=1,largest=False))
print(a.kthvalue(3,dim=1))
print(a.kthvalue(3))
print(a.kthvalue(3,dim=1))
a=torch.randn(4,10)
print(a>0,a.eq(0),a.ge(0))
cond=torch.rand(4,4)
print(cond)
a=torch.zeros(4,4)
b=torch.ones(4,4)
print(torch.where(cond>0.5,a,b))
prob=torch.randn(4,10)
idx=prob.topk(dim=1,k=3)
print(idx)
idx=idx[1]
print(idx)
label=torch.arange(10)+100
print(label)
print(torch.gather(label.expand(4,10),dim=1,index=idx.long()))
x=torch.ones(1)
w=torch.full([1],2)
w.requires_grad_()
mse=torch.nn.functional.mse_loss(torch.ones(1),x*w)
# loss=(1-1*2)**2,dl/dw=2*(1-2)*(-1)=2
print(torch.autograd.grad(mse,[w]))
x=torch.rand(1,10)
w=torch.rand(2,10,requires_grad=True)
o=torch.sigmoid(x@w.t())
print(o.shape)
loss=F.mse_loss(o,torch.ones(1,2))
loss.backward()
# dL/dwjk(j=[0,9],k=[0,1]
print('loss: ',float(loss),w.grad)
x=torch.tensor(7.1)
w1=torch.tensor(2.5,requires_grad=True)
b1=torch.tensor(1)
w2=torch.tensor(3.9,requires_grad=True)
b2=torch.tensor(3)
y1=x*w1+b1
y2=y1*w2+b2
dy2_dy1=torch.autograd.grad(y2,[y1],retain_graph=True)[0]
dy1_dw1=torch.autograd.grad(y1,[w1],retain_graph=True)[0]
dy2_dw1=torch.autograd.grad(y2,[w1],retain_graph=True)[0]
print(dy2_dy1*dy1_dw1)
print(dy2_dw1)
x=torch.randn(1,784)
w=torch.randn(784,10)
logits=x@w
pred=torch.nn.functional.softmax(logits,dim=1)
pred_log=torch.log(pred)
loss=F.nll_loss(pred_log,torch.tensor([3]))
print(loss)
# 传入的参数需为logits，该函数等于softmax+log+nll_loss
loss=F.cross_entropy(logits,torch.tensor([3]))
print(loss)

layer=nn.Conv2d(1,3,kernel_size=3,stride=1,padding=0)
x=torch.rand(1,1,28,28)
out=layer.forward(x)
print(out.shape)
layer=nn.Conv2d(1,3,kernel_size=3,stride=1,padding=1)
out=layer.forward(x)
print(out.shape)
layer=nn.Conv2d(1,3,kernel_size=3,stride=2,padding=1)
out=layer.forward(x)
print(out.shape)
out=layer(x)
print(out.shape)
print(layer.weight,layer.weight.shape,layer.bias.shape)
w=torch.rand(16,3,5,5)
b=torch.rand(16)
x=torch.rand(1,3,28,28)
out=F.conv2d(x,w,b,stride=1,padding=1)
print(out.shape)
out=F.conv2d(x,w,b,stride=2,padding=2)
print(out.shape)
x=torch.rand(1,16,16,16)
layer=nn.MaxPool2d(2,stride=2)
out=layer(x)
print(out.shape)
out=F.avg_pool2d(x,2,stride=2)
print(out.shape)
out=F.interpolate(out,scale_factor=2,mode='nearest')
print(out.shape)
out=F.interpolate(x,scale_factor=3,mode='nearest')
print(out.shape)